{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labor Market Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The standard stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# The world's most amazing NLP library.  Thank you Matthew Honnibal.  Remind me to give you a hug when I see you.\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "\n",
    "# Your classic clustering \n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Ooooooooh - now THIS should be fun.\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "# And make it look pretty!\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "import seaborn as sns\n",
    "\n",
    "# Load SpaCy's large embedding model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and show columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\\\online-job-postings.csv')\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df.groupby('Year')['Title'].count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty cells in target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Title', 'JobDescription', 'Year', 'jobpost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanTitles(sentence):\n",
    "    newsent = ''.join(c for c in sentence if c not in punctuation)\n",
    "    return ' '.join([x for x in newsent.split() if x.lower() not in STOP_WORDS])\n",
    "\n",
    "df['cleanTitle'] = df.apply(lambda row: cleanTitles(row['Title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanTitle'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Clean up Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove description stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopWords(description):\n",
    "    return ' '.join([x for x in description.split() if x.lower() not in STOP_WORDS])\n",
    "\n",
    "df['cleandesc'] = df.apply(lambda row: removeStopWords(row['JobDescription']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleandesc'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count most popular words\n",
    "In this section, we'll find low-signal across all job postings, such as \"job\" and \"performance\", and manually add them to the list.  This is because there are some high-signal words at the top, such as \"software\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldescs = ' '.join(df['cleandesc'].values)\n",
    "alldescs = ' '.join(alldescs.split())\n",
    "\n",
    "wordcount = {}\n",
    "\n",
    "for i in alldescs.lower().split():\n",
    "    if i in wordcount: \n",
    "        wordcount[i] += 1\n",
    "    else:\n",
    "        wordcount[i] = 1\n",
    "        \n",
    "sorted_wordcount = sorted(wordcount.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_wordcount[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this list, we'll select the words that may help in clearing up the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_stopwords = [\n",
    "    'responsible', \n",
    "    'looking', \n",
    "    'incumbent',\n",
    "    'position',\n",
    "    'seeking',\n",
    "    'work',\n",
    "    'support',\n",
    "    'team',\n",
    "    'candidate',\n",
    "    'llc',\n",
    "    'company',\n",
    "    'activities',\n",
    "    'ensure', \n",
    "    'armenian', \n",
    "    'candidates', \n",
    "    '-', \n",
    "    'armenia']\n",
    "\n",
    "def removeCustomStopWords(description):\n",
    "    return ' '.join([x for x in description.split() if x.lower() not in custom_stopwords])\n",
    "\n",
    "df['newcleandesc'] = df.apply(lambda row: removeStopWords(row['cleandesc']), axis=1)\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up jobposts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove jobposts stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopWords(description):\n",
    "    return ' '.join([x for x in description.split() if x.lower() not in STOP_WORDS])\n",
    "\n",
    "df['cleanjob'] = df.apply(lambda row: removeStopWords(row['jobpost']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanjob'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count most popular words\n",
    "In this section, we'll find low-signal across all job postings, such as \"job\" and \"performance\", and manually add them to the list.  This is because there are some high-signal words at the top, such as \"software\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldescs = ' '.join(df['cleanjob'].values)\n",
    "alldescs = ' '.join(alldescs.split())\n",
    "\n",
    "wordcount = {}\n",
    "\n",
    "for i in alldescs.lower().split():\n",
    "    if i in wordcount: \n",
    "        wordcount[i] += 1\n",
    "    else:\n",
    "        wordcount[i] = 1\n",
    "        \n",
    "sorted_wordcount = sorted(wordcount.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_wordcount[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_stopwords = [\n",
    "    '-',\n",
    "    'job', \n",
    "    'application',\n",
    "    'website',\n",
    "    'ability',\n",
    "    'responsible', \n",
    "    'looking', \n",
    "    'incumbent',\n",
    "    'position',\n",
    "    'seeking',\n",
    "    'work',\n",
    "    'support',\n",
    "    'team',\n",
    "    'candidate',\n",
    "    'llc',\n",
    "    'company',\n",
    "    'activities',\n",
    "    'ensure', \n",
    "    'armenian', \n",
    "    'candidates', \n",
    "    '-', \n",
    "    'armenia']\n",
    "\n",
    "def removeCustomStopWords(description):\n",
    "    return ' '.join([x for x in description.split() if x.lower() not in custom_stopwords])\n",
    "\n",
    "df['newcleanjob'] = df.apply(lambda row: removeStopWords(row['cleanjob']), axis=1)\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generate vectors for titles, descriptions, and job posts\n",
    "We are going to create 3 vectors for each job posting: An average word embedding for the title itself, one for the basic description, and because we have a bunch of one-liners, we'll also add the entire post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorsTitles        = []\n",
    "vectorsDescriptions  = []\n",
    "vectorsJobPosts      = []\n",
    "counter = 0 \n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    vectorsTitles.append(nlp(row['cleanTitle']).vector)    \n",
    "    vectorsDescriptions.append(nlp(row['newcleandesc']).vector)\n",
    "    vectorsJobPosts.append(nlp(row['newcleanjob']).vector)\n",
    "    \n",
    "    # show progress every 1000 - maybe go grab some coffee.\n",
    "    counter = counter + 1\n",
    "    if counter % 1000 == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numclusters   = range(1, 20)\n",
    "kmeans        = [KMeans(n_clusters=i) for i in numclusters]\n",
    "\n",
    "\n",
    "kTitles       = [kmeans[i].fit(vectorsTitles) for i in range(len(kmeans))]\n",
    "scoreTitles   = [kTitles[i].score(vectorsTitles) for i in range(len(kmeans))]\n",
    "print('scoreTitles done.')\n",
    "clusterTitles = [kTitles[i].labels_ for i in range(len(kmeans))]\n",
    "print('clusterTitles done.')\n",
    "\n",
    "kDescriptions       = [kmeans[i].fit(vectorsDescriptions) for i in range(len(kmeans))]\n",
    "scoreDescriptions   = [kDescriptions[i].score(vectorsDescriptions) for i in range(len(kmeans))]\n",
    "print('scoreDescriptions done.')\n",
    "clusterDescriptions = [kDescriptions[i].labels_ for i in range(len(kmeans))]\n",
    "print('clusterDescriptions done.')\n",
    "\n",
    "kJobPosts       = [kmeans[i].fit(vectorsJobPosts) for i in range(len(kmeans))]\n",
    "scoreJobPosts   = [kJobPosts[i].score(vectorsJobPosts) for i in range(len(kmeans))]\n",
    "print('scoreJobPosts done.')\n",
    "clusterJobPosts = [kJobPosts[i].labels_ for i in range(len(kmeans))]\n",
    "print('clusterJobPosts done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "fig, axs = plt.subplots(nrows=3, figsize=(15,20))\n",
    "pd.Series([i for i in scoreTitles]      ).plot(ax=axs[0])\n",
    "pd.Series([i for i in scoreDescriptions]).plot(ax=axs[1])\n",
    "pd.Series([i for i in scoreJobPosts]    ).plot(ax=axs[2])\n",
    "axs[0].set_title('Clustering score for job titles')\n",
    "axs[1].set_title('Clustering score for job descriptions')\n",
    "axs[2].set_title('Clustering score for full job posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Find silhouette scores for each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouetteTitles   = [silhouette_score(vectorsTitles, kmeans[i + 1].fit_predict(vectorsTitles)) for i in range(len(kmeans) - 1)]\n",
    "print('silhouetteTitles done.')\n",
    "\n",
    "silhouetteDescriptions   = [silhouette_score(vectorsDescriptions, kmeans[i + 1].fit_predict(vectorsDescriptions)) for i in range(len(kmeans) - 1)]\n",
    "print('silhouetteDescriptions done.')\n",
    "\n",
    "silhouetteJobPosts   = [silhouette_score(vectorsJobPosts, kmeans[i + 1].fit_predict(vectorsJobPosts)) for i in range(len(kmeans) - 1)]\n",
    "print('silhouetteJobPosts done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "silhouetteLabels = [k+1 for (k,v) in enumerate(silhouetteTitles)]\n",
    "fig, axs = plt.subplots(nrows=3, figsize=(15,20))\n",
    "loc = plticker.MultipleLocator(base=1.0)\n",
    "\n",
    "axs[0].plot(silhouetteLabels, silhouetteTitles)\n",
    "axs[0].xaxis.set_major_locator(loc)\n",
    "axs[0].set_title('Silhouette score for job titles')\n",
    "\n",
    "axs[1].plot(silhouetteLabels, silhouetteDescriptions)\n",
    "axs[1].xaxis.set_major_locator(loc)\n",
    "axs[1].set_title('Silhouette score for job descriptions')\n",
    "\n",
    "axs[2].plot(silhouetteLabels, silhouetteJobPosts)\n",
    "axs[2].xaxis.set_major_locator(loc)\n",
    "axs[2].set_title('Silhouette score for full job posts')\n",
    "\n",
    "axs[1].plot([1, 18], [0.05, 0.05], linestyle='dashed', lw=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, those are terrible results.  Let's switch to the big boys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire up UMAP and HDBSCAN\n",
    "Woah, Nelly!  Now we're going to play with the fun stuff.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ideal parameters I've found.  These are the values to experiment with.\n",
    "\n",
    "n_neighbors=40\n",
    "min_cluster_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate arrays \n",
    "newVectors = np.concatenate([vectorsTitles, vectorsDescriptions, vectorsJobPosts], axis=1)\n",
    "np.shape(newVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=n_neighbors)\n",
    "embeddings = reducer.fit_transform(newVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "scoreTitles = hdb.fit(embeddings)\n",
    "print(\"hdb done\")\n",
    "clusterTitles = scoreTitles.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clusterTitles[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's show the pretty clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(nrows=1, figsize=(15,10))\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1],  s=1.5, cmap='rainbow', c=clusterTitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterTitles.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unclustered values: \", list(clusterTitles).count(-1))\n",
    "print(\"Total Values: \", len(list(clusterTitles)))\n",
    "print(\"Data usage: {0:.2f}\".format((len(list(clusterTitles)) - list(clusterTitles).count(-1)) / len(list(clusterTitles))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusterTitles\n",
    "hddf = df\n",
    "hddf['cluster'] = clusterTitles\n",
    "hddf[hddf['cluster'] == 2]['cleanTitle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClusters = clusterTitles.max()\n",
    "hddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hddf.to_csv('jobClusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show most popular words for each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count each word, per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hddf = pd.read_csv('jobClusters.csv')\n",
    "\n",
    "wordcount = {}\n",
    "sorted_wordcount = {}\n",
    "for i in range(max(hddf['cluster'].unique()) + 1):\n",
    "    titles = hddf[hddf['cluster'] == i][\"cleanTitle\"].values\n",
    "    titles = ' '.join(' '.join(titles).split())\n",
    "    wordcount[i] = {}\n",
    "    for j in titles.lower().split():\n",
    "        if j in wordcount[i]: \n",
    "            wordcount[i][j] += 1\n",
    "        else:\n",
    "            wordcount[i][j] = 1\n",
    "        \n",
    "    sorted_wordcount[i] = sorted(wordcount[i].items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Cluster: \" + str(i))\n",
    "    print(sorted_wordcount[i][0:5])\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the top 5 words (to get a sense of the cluster's contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords = {}\n",
    "for key, i in sorted_wordcount.items():\n",
    "    print(\"Cluster \"+str(key)+\": \", end='')\n",
    "    topwords[key] = ''\n",
    "    for newkey, j in sorted_wordcount[key][0:4]:\n",
    "        print(newkey + ' / ', end='')\n",
    "        topwords[key] = topwords[key] + newkey + ' / '\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit of manual cleanup for our purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lawyers\n",
    "#hddf['cluster'] = hddf['cluster'].replace(to_replace=24, value=0, inplace=True, axis=1)\n",
    "\n",
    "def mergeClusters(merge_target, origin):\n",
    "    hddf.loc[hddf['cluster'] == merge_target, 'cluster'] = origin\n",
    "    sorted_wordcount[origin] = sorted_wordcount[merge_target]\n",
    "    sorted_wordcount.pop(merge_target, None)\n",
    "    \n",
    "\n",
    "# Programmers and QA engineers\n",
    "mergeClusters(8, 5)\n",
    "mergeClusters(9, 5)\n",
    "mergeClusters(19, 5)\n",
    "\n",
    "# QA Engineers\n",
    "mergeClusters(18, 14)\n",
    "\n",
    "# Accounting\n",
    "mergeClusters(7, 1)\n",
    "\n",
    "# Executive Assistants\n",
    "mergeClusters(13, 3)\n",
    "\n",
    "# Office Managers and HR\n",
    "mergeClusters(16, 10)\n",
    "\n",
    "# Project Managers\n",
    "mergeClusters(27, 12)\n",
    "\n",
    "# Legal\n",
    "mergeClusters(24, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hddf['cluster'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find top word (for legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsttopword = {}\n",
    "for key, i in sorted_wordcount.items():\n",
    "    print(\"Cluster \"+str(key)+\": \", end='')\n",
    "    firsttopword[key] = [k for k in sorted_wordcount[key][0]][0]\n",
    "    print(firsttopword[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterList = list(hddf['cluster'].unique())\n",
    "clusterList.remove(-1)\n",
    "print(clusterList)\n",
    "\n",
    "datecount = {}\n",
    "for i in clusterList:\n",
    "    dates = hddf[hddf['cluster'] == i]['Year'].values\n",
    "    datecount[i] = {}\n",
    "    for jobdate in dates:\n",
    "        if jobdate in datecount[i]: \n",
    "            datecount[i][jobdate] += 1\n",
    "        else:\n",
    "            datecount[i][jobdate] = 1\n",
    "\n",
    "datedf = pd.DataFrame(datecount)\n",
    "\n",
    "# fill in NaN's\n",
    "datedf.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the view to see the trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = datedf.values \n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "normdf = pd.DataFrame(x_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsttopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(nrows=1, figsize=(20,15))\n",
    "\n",
    "sns.set_palette(\"husl\", 20)  # oooh, pretty\n",
    "\n",
    "datedf.plot(ax=axs)\n",
    "\n",
    "axs.set_title('Job demand over time, per industry')\n",
    "\n",
    "L=plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=int(numClusters/3))\n",
    "for i in clusterList:\n",
    "    L.get_texts()[clusterList.index(i)].set_text(firsttopword[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Find top industries (Top values and fastest growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
